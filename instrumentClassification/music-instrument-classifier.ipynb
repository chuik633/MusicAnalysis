{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-02T12:25:58.890144Z",
     "iopub.status.busy": "2025-01-02T12:25:58.889806Z",
     "iopub.status.idle": "2025-01-02T12:25:58.894176Z",
     "shell.execute_reply": "2025-01-02T12:25:58.893374Z",
     "shell.execute_reply.started": "2025-01-02T12:25:58.890114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow librosa pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes About this file\n",
    "This file uses an existing model and code that I found online. I wasn't sure how to use tensor flow with audio so I worked through this notebook and modified it as I understood it.\n",
    "The main purpose of this file is to create a saved trained_model.pkl that I can load and use to classify the instruments. I explored existing hugging face models for instrument recognition but wanted to have more understanding of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:25:58.907853Z",
     "iopub.status.busy": "2025-01-02T12:25:58.907051Z",
     "iopub.status.idle": "2025-01-02T12:26:12.716481Z",
     "shell.execute_reply": "2025-01-02T12:26:12.715774Z",
     "shell.execute_reply.started": "2025-01-02T12:25:58.907812Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio\n",
    "print(tf.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from pydub import AudioSegment\n",
    "from IPython import display\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import shutil\n",
    "# source tf-env/bin/activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the directory\n",
    "output_dir = './output'\n",
    "input_dir = './data/instrument/musicnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:26:12.718468Z",
     "iopub.status.busy": "2025-01-02T12:26:12.717992Z",
     "iopub.status.idle": "2025-01-02T12:26:14.073649Z",
     "shell.execute_reply": "2025-01-02T12:26:14.072914Z",
     "shell.execute_reply.started": "2025-01-02T12:26:12.718440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear all files in the directory\n",
    "for filename in os.listdir(output_dir):\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        os.remove(file_path) \n",
    "    elif os.path.isdir(file_path):\n",
    "        shutil.rmtree(file_path)  # Remove directory and its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:26:14.074972Z",
     "iopub.status.busy": "2025-01-02T12:26:14.074696Z",
     "iopub.status.idle": "2025-01-02T12:26:18.935407Z",
     "shell.execute_reply": "2025-01-02T12:26:18.934634Z",
     "shell.execute_reply.started": "2025-01-02T12:26:14.074946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "model = hub.load('https://tfhub.dev/google/yamnet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:26:18.936940Z",
     "iopub.status.busy": "2025-01-02T12:26:18.936595Z",
     "iopub.status.idle": "2025-01-02T12:26:18.943366Z",
     "shell.execute_reply": "2025-01-02T12:26:18.942453Z",
     "shell.execute_reply.started": "2025-01-02T12:26:18.936903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Utility functions for loading audio files and making sure the sample rate is correct.\n",
    "@tf.function\n",
    "def load_wav_16k_mono(filename):\n",
    "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    wav, sample_rate = tf.audio.decode_wav(\n",
    "          file_contents,\n",
    "          desired_channels=1)\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:26:18.980969Z",
     "iopub.status.busy": "2025-01-02T12:26:18.980633Z",
     "iopub.status.idle": "2025-01-02T12:26:19.036813Z",
     "shell.execute_reply": "2025-01-02T12:26:19.035940Z",
     "shell.execute_reply.started": "2025-01-02T12:26:18.980933Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech\n",
      "Child speech, kid speaking\n",
      "Conversation\n",
      "Narration, monologue\n",
      "Babbling\n",
      "Speech synthesizer\n",
      "Shout\n",
      "Bellow\n",
      "Whoop\n",
      "Yell\n",
      "Children shouting\n",
      "Screaming\n",
      "Whispering\n",
      "Laughter\n",
      "Baby laughter\n",
      "Giggle\n",
      "Snicker\n",
      "Belly laugh\n",
      "Chuckle, chortle\n",
      "Crying, sobbing\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "class_map_path = model.class_map_path().numpy().decode('utf-8')\n",
    "sound_names =list(pd.read_csv(class_map_path)['display_name'])\n",
    "\n",
    "for name in sound_names[:20]:\n",
    "  print(name)\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:26:19.044270Z",
     "iopub.status.busy": "2025-01-02T12:26:19.043932Z",
     "iopub.status.idle": "2025-01-02T12:26:19.055896Z",
     "shell.execute_reply": "2025-01-02T12:26:19.055158Z",
     "shell.execute_reply.started": "2025-01-02T12:26:19.044232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Mapping of MusicNet instruments and treir indexes\n",
    "musicnet_instruments_map = {\n",
    "1: \"Acoustic Grand Piano\",\n",
    "2: \"Bright Acoustic Piano\",\n",
    "3: \"Electric Grand Piano\",\n",
    "4: \"Honky-tonk Piano\",\n",
    "5: \"Electric Piano 1\",\n",
    "6: \"Electric Piano 2\",\n",
    "7: \"Harpsichord\",\n",
    "8: \"Clavi\",\n",
    "9: \"Celesta\",\n",
    "10: \"Glockenspiel\",\n",
    "11: \"Music Box\",\n",
    "12: \"Vibraphone\",\n",
    "13: \"Marimba\",\n",
    "14: \"Xylophone\",\n",
    "15: \"Tubular Bells\",\n",
    "16: \"Dulcimer\",\n",
    "17: \"Drawbar Organ\",\n",
    "18: \"Percussive Organ\",\n",
    "19: \"Rock Organ\",\n",
    "20: \"Church Organ\",\n",
    "21: \"Reed Organ\",\n",
    "22: \"Accordion\",\n",
    "23: \"Harmonica\",\n",
    "24: \"Tango Accordion\",\n",
    "25: \"Acoustic Guitar (nylon)\",\n",
    "26: \"Acoustic Guitar (steel)\",\n",
    "27: \"Electric Guitar (jazz)\",\n",
    "28: \"Electric Guitar (clean)\",\n",
    "29: \"Electric Guitar (muted)\",\n",
    "30: \"Overdriven Guitar\",\n",
    "31: \"Distortion Guitar\",\n",
    "32: \"Guitar harmonics\",\n",
    "33: \"Acoustic Bass\",\n",
    "34: \"Electric Bass (finger)\",\n",
    "35: \"Electric Bass (pick)\",\n",
    "36: \"Fretless Bass\",\n",
    "37: \"Slap Bass 1\",\n",
    "38: \"Slap Bass 2\",\n",
    "39: \"Synth Bass 1\",\n",
    "40: \"Synth Bass 2\",\n",
    "41: \"Violin\",\n",
    "42: \"Viola\",\n",
    "43: \"Cello\",\n",
    "44: \"Contrabass\",\n",
    "45: \"Tremolo Strings\",\n",
    "46: \"Pizzicato Strings\",\n",
    "47: \"Orchestral Harp\",\n",
    "48: \"Timpani\",\n",
    "49: \"String Ensemble 1\",\n",
    "50: \"String Ensemble 2\",\n",
    "51: \"SynthStrings 1\",\n",
    "52: \"SynthStrings 2\",\n",
    "53: \"Choir Aahs\",\n",
    "54: \"Voice Oohs\",\n",
    "55: \"Synth Voice\",\n",
    "56: \"Orchestra Hit\",\n",
    "57: \"Trumpet\",\n",
    "58: \"Trombone\",\n",
    "59: \"Tuba\",\n",
    "60: \"Muted Trumpet\",\n",
    "61: \"French Horn\",\n",
    "62: \"Brass Section\",\n",
    "63: \"SynthBrass 1\",\n",
    "64: \"SynthBrass 2\",\n",
    "65: \"Soprano Sax\",\n",
    "66: \"Alto Sax\",\n",
    "67: \"Tenor Sax\",\n",
    "68: \"Baritone Sax\",\n",
    "69: \"Oboe\",\n",
    "70: \"English Horn\",\n",
    "71: \"Bassoon\",\n",
    "72: \"Clarinet\",\n",
    "73: \"Piccolo\",\n",
    "74: \"Flute\",\n",
    "75: \"Recorder\",\n",
    "76: \"Pan Flute\",\n",
    "77: \"Blown Bottle\",\n",
    "78: \"Shakuhachi\",\n",
    "79: \"Whistle\",\n",
    "80: \"Ocarina\",\n",
    "81: \"Lead 1 (square)\",\n",
    "82: \"Lead 2 (sawtooth)\",\n",
    "83: \"Lead 3 (calliope)\",\n",
    "84: \"Lead 4 (chiff)\",\n",
    "85: \"Lead 5 (charang)\",\n",
    "86: \"Lead 6 (voice)\",\n",
    "87: \"Lead 7 (fifths)\",\n",
    "88: \"Lead 8 (bass + lead)\",\n",
    "89: \"Pad 1 (new age)\",\n",
    "90: \"Pad 2 (warm)\",\n",
    "91: \"Pad 3 (polysynth)\",\n",
    "92: \"Pad 4 (choir)\",\n",
    "93: \"Pad 5 (bowed)\",\n",
    "94: \"Pad 6 (metallic)\",\n",
    "95: \"Pad 7 (halo)\",\n",
    "96: \"Pad 8 (sweep)\",\n",
    "97: \"FX 1 (rain)\",\n",
    "98: \"FX 2 (soundtrack)\",\n",
    "99: \"FX 3 (crystal)\",\n",
    "100: \"FX 4 (atmosphere)\",\n",
    "101: \"FX 5 (brightness)\",\n",
    "102: \"FX 6 (goblins)\",\n",
    "103: \"FX 7 (echoes)\",\n",
    "104: \"FX 8 (sci-fi)\",\n",
    "105: \"Sitar\",\n",
    "106: \"Banjo\",\n",
    "107: \"Shamisen\",\n",
    "108: \"Koto\",\n",
    "109: \"Kalimba\",\n",
    "110: \"Bag pipe\",\n",
    "111: \"Fiddle\",\n",
    "112: \"Shanai\",\n",
    "113: \"Tinkle Bell\",\n",
    "114: \"Agogo\",\n",
    "115: \"Steel Drums\",\n",
    "116: \"Woodblock\",\n",
    "117: \"Taiko Drum\",\n",
    "118: \"Melodic Tom\",\n",
    "119: \"Synth Drum\",\n",
    "120: \"Reverse Cymbal\",\n",
    "121: \"Guitar Fret Noise\",\n",
    "122: \"Breath Noise\",\n",
    "123: \"Seashore\",\n",
    "124: \"Bird Tweet\",\n",
    "125: \"Telephone Ring\",\n",
    "126: \"Helicopter\",\n",
    "127: \"Applause\",\n",
    "128: \"Gunshot\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:26:19.057446Z",
     "iopub.status.busy": "2025-01-02T12:26:19.057063Z",
     "iopub.status.idle": "2025-01-02T12:26:22.040884Z",
     "shell.execute_reply": "2025-01-02T12:26:22.039979Z",
     "shell.execute_reply.started": "2025-01-02T12:26:19.057408Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 69, 7, 72, 41, 42, 43, 71, 74, 44, 61}\n"
     ]
    }
   ],
   "source": [
    "def update_unique_instrument_indexs(file_directory, unique_instruments):\n",
    "    for filename in os.listdir(file_directory):\n",
    "        if filename.endswith('.csv'):  \n",
    "            file_path = os.path.join(file_directory, filename)\n",
    "            df = pd.read_csv(file_path) \n",
    "            \n",
    "            unique_instruments.update(df['instrument'].unique())\n",
    "\n",
    "train_labels_directory = input_dir+'/train_labels'\n",
    "test_labels_directory = input_dir+'/test_labels'\n",
    "\n",
    "unique_instrument_indexes = set()\n",
    "\n",
    "update_unique_instrument_indexs(train_labels_directory, unique_instrument_indexes)\n",
    "update_unique_instrument_indexs(test_labels_directory, unique_instrument_indexes)\n",
    "\n",
    "print(unique_instrument_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:26:22.042145Z",
     "iopub.status.busy": "2025-01-02T12:26:22.041894Z",
     "iopub.status.idle": "2025-01-02T12:26:22.047129Z",
     "shell.execute_reply": "2025-01-02T12:26:22.046173Z",
     "shell.execute_reply.started": "2025-01-02T12:26:22.042120Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Acoustic Grand Piano', 7: 'Harpsichord', 41: 'Violin', 42: 'Viola', 43: 'Cello', 44: 'Contrabass', 61: 'French Horn', 69: 'Oboe', 71: 'Bassoon', 72: 'Clarinet', 74: 'Flute'}\n"
     ]
    }
   ],
   "source": [
    "filtered_musicnet_instruments_map = {k: v for k, v in musicnet_instruments_map.items() if k in unique_instrument_indexes}\n",
    "\n",
    "print(filtered_musicnet_instruments_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:26:22.048794Z",
     "iopub.status.busy": "2025-01-02T12:26:22.048453Z",
     "iopub.status.idle": "2025-01-02T12:26:22.061636Z",
     "shell.execute_reply": "2025-01-02T12:26:22.060780Z",
     "shell.execute_reply.started": "2025-01-02T12:26:22.048758Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped Instruments:\n",
      "1: Piano\n",
      "7: Harpsichord\n",
      "41: Violin, fiddle\n",
      "43: Cello\n",
      "61: French horn\n",
      "72: Clarinet\n",
      "74: Flute\n",
      "{1: 'Piano', 7: 'Harpsichord', 41: 'Violin, fiddle', 43: 'Cello', 61: 'French horn', 72: 'Clarinet', 74: 'Flute'}\n"
     ]
    }
   ],
   "source": [
    "mapped_instruments = {}\n",
    "\n",
    "# Simplify piano label to be able to mach the correct soundname from Yamnet\n",
    "filtered_musicnet_instruments_map[1] = 'Piano'\n",
    "\n",
    "for instrument_id, instrument_name in filtered_musicnet_instruments_map.items():\n",
    "    # Iterate through sound categories to find a match\n",
    "    for sound_category in sound_names:\n",
    "        if instrument_name.lower() in sound_category.lower():\n",
    "            mapped_instruments[instrument_id] = sound_category\n",
    "            break\n",
    "\n",
    "print(\"Mapped Instruments:\")\n",
    "for key, value in mapped_instruments.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(mapped_instruments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:26:22.066526Z",
     "iopub.status.busy": "2025-01-02T12:26:22.066233Z",
     "iopub.status.idle": "2025-01-02T12:26:22.076695Z",
     "shell.execute_reply": "2025-01-02T12:26:22.075748Z",
     "shell.execute_reply.started": "2025-01-02T12:26:22.066501Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped Sounds:\n",
      "1: Piano\n",
      "7: Harpsichord\n",
      "41: Violin, fiddle\n",
      "43: Cello\n",
      "61: French horn\n",
      "72: Clarinet\n",
      "74: Flute\n",
      "42: Violin, fiddle\n",
      "44: Double bass\n",
      "69: Wind instrument, woodwind instrument\n",
      "71: Wind instrument, woodwind instrument\n"
     ]
    }
   ],
   "source": [
    "# Manulally map instruments which names doesn't match to ones in Yamnet model sound names\n",
    "mapped_instruments[42] = 'Violin, fiddle'\n",
    "mapped_instruments[44] = 'Double bass'\n",
    "mapped_instruments[69] = 'Wind instrument, woodwind instrument'\n",
    "mapped_instruments[71] = 'Wind instrument, woodwind instrument'\n",
    "\n",
    "# Print the resulting mapping\n",
    "print(\"Mapped Sounds:\")\n",
    "for index, name in mapped_instruments.items():\n",
    "    print(f\"{index}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:26:22.077997Z",
     "iopub.status.busy": "2025-01-02T12:26:22.077752Z",
     "iopub.status.idle": "2025-01-02T12:26:27.279402Z",
     "shell.execute_reply": "2025-01-02T12:26:27.278420Z",
     "shell.execute_reply.started": "2025-01-02T12:26:22.077973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Add a target column to each file and save the results in the output directory\n",
    "def process_csv_files(input_directory, output_directory, mapped_instruments):\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith('.csv'):  \n",
    "            file_path = os.path.join(input_directory, filename)\n",
    "            df = pd.read_csv(file_path)  \n",
    "            \n",
    "            # Add a target column containing the mapped instrument name\n",
    "            df['target'] = df['instrument'].map(mapped_instruments)\n",
    "            \n",
    "            updated_file_path = os.path.join(output_directory, filename)\n",
    "            df.to_csv(updated_file_path, index=False)\n",
    "\n",
    "train_labels_directory = input_dir+'/train_labels'\n",
    "test_labels_directory = input_dir+'/test_labels'\n",
    "\n",
    "output_train_labels_directory = output_dir +'/labels/train_labels'\n",
    "output_test_labels_directory = output_dir +'/labels/test_labels'\n",
    "\n",
    "process_csv_files(train_labels_directory, output_train_labels_directory, mapped_instruments)\n",
    "process_csv_files(test_labels_directory, output_test_labels_directory, mapped_instruments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:26:27.281419Z",
     "iopub.status.busy": "2025-01-02T12:26:27.280986Z",
     "iopub.status.idle": "2025-01-02T12:34:16.928896Z",
     "shell.execute_reply": "2025-01-02T12:34:16.927918Z",
     "shell.execute_reply.started": "2025-01-02T12:26:27.281367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_to_pcm(input_path, output_path):\n",
    "    \"\"\"Convert an audio file to 16-bit PCM WAV format with 16 kHz sample rate and mono channel.\"\"\"\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        audio = AudioSegment.from_file(input_path)\n",
    "        \n",
    "        # Ensure the audio is set to 16 kHz, mono, and 16-bit\n",
    "        audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)  # 16 kHz, mono, 16-bit\n",
    "        \n",
    "        # Export the audio as PCM WAV format\n",
    "        audio.export(output_path, format=\"wav\")\n",
    "        print(f\"Successfully converted {input_path} to {output_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {input_path}: {e}\")\n",
    "        return None  # Return None if conversion fails\n",
    "\n",
    "    return output_path\n",
    "\n",
    "def batch_convert_to_pcm(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Convert all audio files in a directory to PCM WAV format.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith(('.wav')):\n",
    "            input_file = os.path.join(input_dir, file_name)\n",
    "            output_file = os.path.join(output_dir, f\"{os.path.splitext(file_name)[0]}_pcm.wav\")\n",
    "            \n",
    "            # Convert the file and save the PCM version\n",
    "            pcm_file = convert_to_pcm(input_file, output_file)\n",
    "            if pcm_file is None:\n",
    "                print(f\"Skipping file: {file_name} due to conversion error.\")\n",
    "                \n",
    "input_test_directory =input_dir+ \"/test_data\"\n",
    "output_test_directory = output_dir + \"/converted_audio/test_data\"\n",
    "batch_convert_to_pcm(input_test_directory, output_test_directory)\n",
    "\n",
    "input_train_directory = input_dir+ \"/train_data\"\n",
    "output_train_directory = output_dir + \"/converted_audio/training_data\"\n",
    "batch_convert_to_pcm(input_train_directory, output_train_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:34:29.714619Z",
     "iopub.status.busy": "2025-01-02T12:34:29.714114Z",
     "iopub.status.idle": "2025-01-02T12:34:29.720268Z",
     "shell.execute_reply": "2025-01-02T12:34:29.719367Z",
     "shell.execute_reply.started": "2025-01-02T12:34:29.714576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_audio_into_chunks(audio, chunk_size=3, sample_rate=16000):\n",
    "    \"\"\"Split audio into chunks of specified duration in seconds.\"\"\"\n",
    "    samples_per_chunk = chunk_size * sample_rate\n",
    "    num_chunks = len(audio) // samples_per_chunk\n",
    "    return np.array_split(audio, num_chunks + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:34:29.721606Z",
     "iopub.status.busy": "2025-01-02T12:34:29.721375Z",
     "iopub.status.idle": "2025-01-02T12:34:29.735823Z",
     "shell.execute_reply": "2025-01-02T12:34:29.734978Z",
     "shell.execute_reply.started": "2025-01-02T12:34:29.721583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_embeddings(audio_chunk, yamnet_model):\n",
    "    \"\"\"Extract embeddings from a chunk of audio.\"\"\"\n",
    "    audio_tensor = tf.convert_to_tensor(audio_chunk, dtype=tf.float32)  # Convert chunk to tensor\n",
    "    scores, embeddings, _ = yamnet_model(audio_tensor)  # No need to expand dimensions\n",
    "    return embeddings.numpy(), scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:34:29.737110Z",
     "iopub.status.busy": "2025-01-02T12:34:29.736843Z",
     "iopub.status.idle": "2025-01-02T12:34:29.749169Z",
     "shell.execute_reply": "2025-01-02T12:34:29.748219Z",
     "shell.execute_reply.started": "2025-01-02T12:34:29.737068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_wav_16k_mono(filename):\n",
    "    \"\"\"\n",
    "    Load an audio file, convert it to mono, and resample to 16 kHz.\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(filename, sr=16000, mono=True)\n",
    "    return audio\n",
    "\n",
    "def process_and_save_embeddings(dataset_path, yamnet_model, output_dir, chunk_size=3):\n",
    "    \"\"\"Process each audio file in the dataset and save embeddings.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    audio_files = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith('.wav')]\n",
    "\n",
    "    for file_path in audio_files:\n",
    "        print(f\"Processing {file_path}...\")\n",
    "        audio = load_wav_16k_mono(file_path)  # Use the librosa-based function\n",
    "        chunks = split_audio_into_chunks(audio, chunk_size=chunk_size)\n",
    "        file_results = []\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if len(chunk) == 0:\n",
    "                continue\n",
    "            embeddings, scores = extract_embeddings(chunk, yamnet_model)\n",
    "            file_results.append({\n",
    "                \"chunk_index\": i,\n",
    "                \"embeddings\": embeddings,\n",
    "                \"scores\": scores,\n",
    "            })\n",
    "\n",
    "        output_file = os.path.join(output_dir, f\"{os.path.basename(file_path)}_embeddings.pkl\")\n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(file_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T12:34:29.750598Z",
     "iopub.status.busy": "2025-01-02T12:34:29.750289Z",
     "iopub.status.idle": "2025-01-02T12:43:00.834173Z",
     "shell.execute_reply": "2025-01-02T12:43:00.833418Z",
     "shell.execute_reply.started": "2025-01-02T12:34:29.750574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "training_data_path = output_dir+'/converted_audio/training_data'\n",
    "training_output_dir = output_dir+'/embeddings/training_data'\n",
    "\n",
    "test_data_path = output_dir+'/converted_audio/test_data'\n",
    "test_output_dir =  output_dir+'/embeddings/test_data'\n",
    "\n",
    "process_and_save_embeddings(training_data_path, model, training_output_dir, chunk_size=3)\n",
    "process_and_save_embeddings(test_data_path, model, test_output_dir, chunk_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T13:20:38.003287Z",
     "iopub.status.busy": "2025-01-02T13:20:38.002922Z",
     "iopub.status.idle": "2025-01-02T13:20:38.014148Z",
     "shell.execute_reply": "2025-01-02T13:20:38.013146Z",
     "shell.execute_reply.started": "2025-01-02T13:20:38.003257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to align embeddings with labels\n",
    "def align_embeddings_with_labels(embeddings, labels, target_column, class_list):\n",
    "    # Combine all embeddings (average across time/chunks)\n",
    "    combined_embeddings = np.vstack([chunk.mean(axis=0) for chunk in embeddings])  # Average per chunk\n",
    "    \n",
    "    # Ensure the number of embeddings matches the number of labels\n",
    "    min_length = min(len(combined_embeddings), len(labels))\n",
    "    combined_embeddings = combined_embeddings[:min_length]\n",
    "    labels = labels.iloc[:min_length]\n",
    "    \n",
    "    # Create a binary matrix for the target column using the unified class list\n",
    "    label_binarized = pd.get_dummies(labels[target_column])\n",
    "    label_binarized = label_binarized.reindex(columns=class_list, fill_value=0).astype(int).values\n",
    "    \n",
    "    return combined_embeddings, label_binarized\n",
    "\n",
    "# Preprocess all data\n",
    "def preprocess_yamnet_data(embeddings_dir, labels_dir, target_column='instrument', class_list=None):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # List all embedding and label files\n",
    "    embedding_files = sorted([os.path.join(embeddings_dir, f) for f in os.listdir(embeddings_dir) if f.endswith('.pkl')])\n",
    "    label_files = sorted([os.path.join(labels_dir, f) for f in os.listdir(labels_dir) if f.endswith('.csv')])\n",
    "    \n",
    "    # If no class_list is provided (for training), create it\n",
    "    if class_list is None:\n",
    "        class_set = set()\n",
    "        for lbl_file in label_files:\n",
    "            labels = pd.read_csv(lbl_file)\n",
    "            class_set.update(labels[target_column].unique())\n",
    "        class_list = sorted(class_set)  # Sort for consistent ordering\n",
    "    \n",
    "    # Process embeddings and labels\n",
    "    for emb_file, lbl_file in zip(embedding_files, label_files):\n",
    "        # Load embeddings\n",
    "        with open(emb_file, 'rb') as f:\n",
    "            embeddings = pickle.load(f)\n",
    "        embedding_chunks = [chunk['embeddings'] for chunk in embeddings]\n",
    "        \n",
    "        # Load and align labels\n",
    "        labels = pd.read_csv(lbl_file)\n",
    "        combined_embeddings = np.vstack([chunk.mean(axis=0) for chunk in embedding_chunks])\n",
    "        min_length = min(len(combined_embeddings), len(labels))\n",
    "        combined_embeddings = combined_embeddings[:min_length]\n",
    "        labels = labels.iloc[:min_length]\n",
    "        \n",
    "        # Create binary label matrix aligned with the class_list\n",
    "        label_binarized = pd.get_dummies(labels[target_column])\n",
    "        label_binarized = label_binarized.reindex(columns=class_list, fill_value=0).values\n",
    "        \n",
    "        all_embeddings.append(combined_embeddings)\n",
    "        all_labels.append(label_binarized)\n",
    "    \n",
    "    # Combine all processed data\n",
    "    X = np.vstack(all_embeddings)\n",
    "    y = np.vstack(all_labels)\n",
    "    return X, y, class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:19:11.013933Z",
     "iopub.status.busy": "2025-01-02T16:19:11.013130Z",
     "iopub.status.idle": "2025-01-02T16:19:15.910140Z",
     "shell.execute_reply": "2025-01-02T16:19:15.909068Z",
     "shell.execute_reply.started": "2025-01-02T16:19:11.013897Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (40625, 1024)\n",
      "Number of labels: 40625\n",
      "Classes: [1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = output_dir+'//embeddings/training_data'\n",
    "train_labels = output_dir+'/labels/train_labels'\n",
    "X_train, y_train, classes = preprocess_yamnet_data(train_embeddings, train_labels)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train.shape}\")\n",
    "print(f\"Number of labels: {len(y_train)}\")\n",
    "print(f\"Classes: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:21:39.797657Z",
     "iopub.status.busy": "2025-01-02T16:21:39.796507Z",
     "iopub.status.idle": "2025-01-02T16:21:39.864679Z",
     "shell.execute_reply": "2025-01-02T16:21:39.863526Z",
     "shell.execute_reply.started": "2025-01-02T16:21:39.797602Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (499, 1024)\n",
      "Number of labels: 499\n",
      "Classes: [1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74]\n",
      "Aligned Feature matrix shape: (499, 1024)\n",
      "Aligned Label matrix shape: (499, 11)\n",
      "Classes (aligned to training): [1, 7, 41, 42, 43, 44, 61, 69, 71, 72, 74]\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = output_dir+'/embeddings/test_data'\n",
    "test_labels = output_dir+'/labels/test_labels'\n",
    "X_test, y_test,classes_test = preprocess_yamnet_data(test_embeddings, test_labels, target_column='instrument', class_list=classes)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_test.shape}\")\n",
    "print(f\"Number of labels: {len(y_test)}\")\n",
    "print(f\"Classes: {classes_test}\")\n",
    "\n",
    "# Ensure alignment of y_test with training classes\n",
    "import pandas as pd\n",
    "y_test = pd.DataFrame(y_test, columns=classes[:len(y_test[0])]).reindex(columns=classes, fill_value=0).values\n",
    "\n",
    "print(f\"Aligned Feature matrix shape: {X_test.shape}\")\n",
    "print(f\"Aligned Label matrix shape: {y_test.shape}\")\n",
    "print(f\"Classes (aligned to training): {classes_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T13:20:50.119740Z",
     "iopub.status.busy": "2025-01-02T13:20:50.119350Z",
     "iopub.status.idle": "2025-01-02T13:20:50.283043Z",
     "shell.execute_reply": "2025-01-02T13:20:50.282053Z",
     "shell.execute_reply.started": "2025-01-02T13:20:50.119710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save as files to avoid processing each time\n",
    "save_dir = output_dir+'/processed_embeddings/'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "np.save(save_dir + 'X_train.npy', X_train)\n",
    "np.save(save_dir + 'y_train.npy', y_train)\n",
    "np.save(save_dir + 'classes.npy', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T13:20:59.845958Z",
     "iopub.status.busy": "2025-01-02T13:20:59.845598Z",
     "iopub.status.idle": "2025-01-02T13:20:59.854044Z",
     "shell.execute_reply": "2025-01-02T13:20:59.853100Z",
     "shell.execute_reply.started": "2025-01-02T13:20:59.845926Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save as files to avoid processing each time\n",
    "np.save(save_dir + 'X_test.npy', X_test)\n",
    "np.save(save_dir + 'y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T13:22:36.821437Z",
     "iopub.status.busy": "2025-01-02T13:22:36.821004Z",
     "iopub.status.idle": "2025-01-02T13:35:47.294482Z",
     "shell.execute_reply": "2025-01-02T13:35:47.293522Z",
     "shell.execute_reply.started": "2025-01-02T13:22:36.821403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Initialize and train the model\n",
    "clf = OneVsRestClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "\n",
    "# Convert all boolean values to integers\n",
    "y_train = np.array(y_train, dtype=int)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we can save the model so we dont have to train it each time. later we will reload it to show how that works.\n",
    "with open(\"trained_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved model\n",
    "with open(\"trained_model.pkl\", \"rb\") as file:\n",
    "    clf = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T16:21:54.985502Z",
     "iopub.status.busy": "2025-01-02T16:21:54.985085Z",
     "iopub.status.idle": "2025-01-02T16:21:55.184648Z",
     "shell.execute_reply": "2025-01-02T16:21:55.183634Z",
     "shell.execute_reply.started": "2025-01-02T16:21:54.985467Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Piano', 7: 'Harpsichord', 41: 'Violin, fiddle', 43: 'Cello', 61: 'French horn', 72: 'Clarinet', 74: 'Flute', 42: 'Violin, fiddle', 44: 'Double bass', 69: 'Wind instrument, woodwind instrument', 71: 'Wind instrument, woodwind instrument'}\n",
      "Aligned y_test shape: (499, 11)\n",
      "y_pred shape: (499, 11)\n",
      "Unique values in y_test: [0 1]\n",
      "Unique values in y_pred: [0 1]\n",
      "Hamming Loss: 0.04\n",
      "Classification Report:\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "                               Piano       0.92      0.94      0.93       179\n",
      "                         Harpsichord       0.00      0.00      0.00         0\n",
      "                      Violin, fiddle       0.79      0.58      0.67       118\n",
      "                               Cello       0.00      0.00      0.00        22\n",
      "                         French horn       0.96      0.68      0.80        73\n",
      "                            Clarinet       0.00      0.00      0.00         0\n",
      "                               Flute       0.00      0.00      0.00        40\n",
      "                      Violin, fiddle       0.00      0.00      0.00         0\n",
      "                         Double bass       1.00      0.04      0.08        23\n",
      "Wind instrument, woodwind instrument       0.00      0.00      0.00        44\n",
      "Wind instrument, woodwind instrument       0.00      0.00      0.00         0\n",
      "\n",
      "                           micro avg       0.89      0.58      0.70       499\n",
      "                           macro avg       0.33      0.20      0.23       499\n",
      "                        weighted avg       0.71      0.58      0.61       499\n",
      "                         samples avg       0.58      0.58      0.58       499\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katherine/Desktop/DataVisualization/ML Class/tf-env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/katherine/Desktop/DataVisualization/ML Class/tf-env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/katherine/Desktop/DataVisualization/ML Class/tf-env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/katherine/Desktop/DataVisualization/ML Class/tf-env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import classification_report\n",
    "class_names_array = list(mapped_instruments.keys())\n",
    "print(mapped_instruments)\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Ensure consistent shapes\n",
    "print(f\"Aligned y_test shape: {y_test.shape}\")\n",
    "print(f\"y_pred shape: {y_pred.shape}\")\n",
    "\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "print(\"Unique values in y_test:\", np.unique(y_test))\n",
    "print(\"Unique values in y_pred:\", np.unique(y_pred))\n",
    "\n",
    "# Hamming Loss\n",
    "print(f\"Hamming Loss: {hamming_loss(y_test, y_pred):.2f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[mapped_instruments[cls] for cls in class_names_array]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T17:39:29.936973Z",
     "iopub.status.busy": "2025-01-02T17:39:29.936621Z",
     "iopub.status.idle": "2025-01-02T17:39:30.012502Z",
     "shell.execute_reply": "2025-01-02T17:39:30.011568Z",
     "shell.execute_reply.started": "2025-01-02T17:39:29.936942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_single_yamnet_file(embedding_file, label_file, target_column='instrument', class_list=None):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    \n",
    "    # Load embeddings\n",
    "    with open(embedding_file, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    embedding_chunks = [chunk['embeddings'] for chunk in embeddings]\n",
    "    \n",
    "    # Load labels\n",
    "    labels = pd.read_csv(label_file)\n",
    "    \n",
    "    # If no class_list is provided, infer it from the label file\n",
    "    if class_list is None:\n",
    "        class_list = sorted(labels[target_column].unique())  # Sort for consistent ordering\n",
    "    \n",
    "    # Align embeddings with labels\n",
    "    combined_embeddings, label_binarized = align_embeddings_with_labels(\n",
    "        embedding_chunks, labels, target_column, class_list\n",
    "    )\n",
    "    \n",
    "    return combined_embeddings, label_binarized, class_list\n",
    "\n",
    "fileName = output_dir+'/embeddings/test_data/1759_pcm.wav_embeddings.pkl'\n",
    "labelFileName = output_dir+'/labels/test_labels/1759.csv'\n",
    "x_values, y_values, class_list = preprocess_single_yamnet_file(fileName,labelFileName, 'instrument', classes)\n",
    "\n",
    "y_pred = clf.predict(x_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T17:39:35.986673Z",
     "iopub.status.busy": "2025-01-02T17:39:35.986323Z",
     "iopub.status.idle": "2025-01-02T17:39:35.999153Z",
     "shell.execute_reply": "2025-01-02T17:39:35.998314Z",
     "shell.execute_reply.started": "2025-01-02T17:39:35.986644Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pedicted instruments: Piano\n",
      "Correct instriments: Piano\n",
      "Chunk 1: Predicted: Piano | Correct: Piano\n",
      "Chunk 2: Predicted: Piano | Correct: Piano\n",
      "Chunk 3: Predicted: Piano | Correct: Piano\n",
      "Chunk 4: Predicted: Piano | Correct: Piano\n",
      "Chunk 5: Predicted: Piano | Correct: Piano\n",
      "Chunk 6: Predicted: Piano | Correct: Piano\n",
      "Chunk 7: Predicted: Piano | Correct: Piano\n",
      "Chunk 8: Predicted: Piano | Correct: Piano\n",
      "Chunk 9: Predicted: Piano | Correct: Piano\n",
      "Chunk 10: Predicted: Piano | Correct: Piano\n",
      "Chunk 11: Predicted: Piano | Correct: Piano\n",
      "Chunk 12: Predicted: Piano | Correct: Piano\n",
      "Chunk 13: Predicted: Piano | Correct: Piano\n",
      "Chunk 14: Predicted: Piano | Correct: Piano\n",
      "Chunk 15: Predicted: Piano | Correct: Piano\n",
      "Chunk 16: Predicted: Piano | Correct: Piano\n",
      "Chunk 17: Predicted: Piano | Correct: Piano\n",
      "Chunk 18: Predicted: Piano | Correct: Piano\n",
      "Chunk 19: Predicted: Piano | Correct: Piano\n",
      "Chunk 20: Predicted: Piano | Correct: Piano\n",
      "Chunk 21: Predicted: Piano | Correct: Piano\n",
      "Chunk 22: Predicted: Piano | Correct: Piano\n",
      "Chunk 23: Predicted: Piano | Correct: Piano\n",
      "Chunk 24: Predicted: Piano | Correct: Piano\n",
      "Chunk 25: Predicted: Piano | Correct: Piano\n",
      "Chunk 26: Predicted: Piano | Correct: Piano\n",
      "Chunk 27: Predicted: Piano | Correct: Piano\n",
      "Chunk 28: Predicted: Piano | Correct: Piano\n",
      "Chunk 29: Predicted: Piano | Correct: Piano\n",
      "Chunk 30: Predicted: Piano | Correct: Piano\n",
      "Chunk 31: Predicted: Piano | Correct: Piano\n",
      "Chunk 32: Predicted: Piano | Correct: Piano\n",
      "Chunk 33: Predicted: Piano | Correct: Piano\n",
      "Chunk 34: Predicted: Piano | Correct: Piano\n",
      "Chunk 35: Predicted: Piano | Correct: Piano\n",
      "Chunk 36: Predicted: Piano | Correct: Piano\n",
      "Chunk 37: Predicted: Piano | Correct: Piano\n",
      "Chunk 38: Predicted: Piano | Correct: Piano\n",
      "Chunk 39: Predicted: Piano | Correct: Piano\n",
      "Chunk 40: Predicted: Piano | Correct: Piano\n",
      "Chunk 41: Predicted: Piano | Correct: Piano\n",
      "Chunk 42: Predicted: Piano | Correct: Piano\n",
      "Chunk 43: Predicted: Piano | Correct: Piano\n",
      "Chunk 44: Predicted: Piano | Correct: Piano\n",
      "Chunk 45: Predicted: Piano | Correct: Piano\n",
      "Chunk 46: Predicted: Piano | Correct: Piano\n",
      "Chunk 47: Predicted: Piano | Correct: Piano\n",
      "Chunk 48: Predicted: Piano | Correct: Piano\n",
      "Chunk 49: Predicted: Piano | Correct: Piano\n",
      "Chunk 50: Predicted: Piano | Correct: Piano\n",
      "Chunk 51: Predicted: Piano | Correct: Piano\n",
      "Chunk 52: Predicted: Piano | Correct: Piano\n",
      "Chunk 53: Predicted: Piano | Correct: Piano\n",
      "Chunk 54: Predicted: Piano | Correct: Piano\n",
      "Chunk 55: Predicted: Piano | Correct: Piano\n",
      "Chunk 56: Predicted: Piano | Correct: Piano\n",
      "Chunk 57: Predicted: Piano | Correct: Piano\n",
      "Chunk 58: Predicted: Piano | Correct: Piano\n",
      "Chunk 59: Predicted: Piano | Correct: Piano\n",
      "Chunk 60: Predicted: Piano | Correct: Piano\n",
      "Chunk 61: Predicted: Piano | Correct: Piano\n",
      "Chunk 62: Predicted: Piano | Correct: Piano\n",
      "Chunk 63: Predicted: Piano | Correct: Piano\n",
      "Chunk 64: Predicted: Piano | Correct: Piano\n",
      "Chunk 65: Predicted: Piano | Correct: Piano\n"
     ]
    }
   ],
   "source": [
    "# Convert each binary vector to a list of class names\n",
    "def get_instruments(y_values, classes, mapped_instruments):\n",
    "    sample_by_sample = []\n",
    "    all_instruments_set = set()  # To collect all instruments across the whole dataset\n",
    "    \n",
    "    for row in y_values:\n",
    "        # Find indices where the value is 1 (class presence)\n",
    "        class_indices = np.where(row == 1)[0]\n",
    "\n",
    "        # Get the corresponding instrument indexes\n",
    "        instrument_indexes = [classes[idx] for idx in class_indices]\n",
    "\n",
    "        # Map the indexes to instrument names and get unique names\n",
    "        instrument_names = np.unique([mapped_instruments[idx] for idx in instrument_indexes])\n",
    "        \n",
    "        # Add instrument names to the human-readable output for the current row\n",
    "        sample_by_sample.append(instrument_names)\n",
    "\n",
    "        # Add the instrument names to the aggregated set (avoiding duplicates)\n",
    "        all_instruments_set.update(instrument_names)\n",
    "    \n",
    "    # Convert the set to a sorted list\n",
    "    aggregated_instruments = sorted(list(all_instruments_set))\n",
    "\n",
    "    return sample_by_sample, aggregated_instruments\n",
    "\n",
    "sample_by_sample_predicted, aggregated_predicted_instruments = get_instruments(y_pred,classes,mapped_instruments)\n",
    "sample_by_sample_values, aggregated_instruments = get_instruments(y_values,classes,mapped_instruments)\n",
    "\n",
    "print(\"Pedicted instruments:\",', '.join(aggregated_predicted_instruments))\n",
    "print(\"Correct instriments:\",', '.join(aggregated_instruments))\n",
    "\n",
    "\n",
    "for idx, (predicted, correct) in enumerate(zip(sample_by_sample_predicted, sample_by_sample_values)):\n",
    "    # Print predicted value\n",
    "    predicted_value = predicted[0] if predicted.size > 0 else \"No instrument\"\n",
    "    \n",
    "    # Print correct value\n",
    "    correct_value = correct[0] if correct.size > 0 else \"No instrument\"\n",
    "    \n",
    "    print(f\"Chunk {idx + 1}: Predicted: {predicted_value} | Correct: {correct_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained the model we can pickle and save it so that we can use it again without going through the training process from start.\n",
    "Below I show how we can use it on different test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pedicted instruments: Piano, Wind instrument, woodwind instrument\n",
      "Correct instriments: Clarinet, French horn, Wind instrument, woodwind instrument\n"
     ]
    }
   ],
   "source": [
    "num = '1819'\n",
    "fileName = output_dir+f'/embeddings/test_data/{num}_pcm.wav_embeddings.pkl'\n",
    "labelFileName = output_dir+f'/labels/test_labels/{num}.csv'\n",
    "\n",
    "#preprocess\n",
    "x_values, y_values, class_list = preprocess_single_yamnet_file(fileName,labelFileName, 'instrument', classes)\n",
    "\n",
    "#predict\n",
    "y_pred = clf.predict(x_values)\n",
    "\n",
    "#preview\n",
    "sample_by_sample_predicted, aggregated_predicted_instruments = get_instruments(y_pred,classes,mapped_instruments)\n",
    "sample_by_sample_values, aggregated_instruments = get_instruments(y_values,classes,mapped_instruments)\n",
    "\n",
    "print(\"Pedicted instruments:\",', '.join(aggregated_predicted_instruments))\n",
    "print(\"Correct instriments:\",', '.join(aggregated_instruments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted instruments: Piano, Wind instrument, woodwind instrument\n",
      "Predicted: No instrument\n"
     ]
    }
   ],
   "source": [
    "# use the model to predict the instrument of a single audio file\n",
    "def predict_instrument(file_path, model, yamnet_model, mapped_instruments, chunk_size=3):\n",
    "    # Load and preprocess the audio file\n",
    "    audio = load_wav_16k_mono(file_path)\n",
    "    chunks = split_audio_into_chunks(audio, chunk_size=chunk_size)\n",
    "    \n",
    "    # Extract embeddings\n",
    "    all_embeddings = []\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "        embeddings, _ = extract_embeddings(chunk, yamnet_model)\n",
    "        all_embeddings.append(embeddings.mean(axis=0))  # Average across time\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    combined_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    # Predict using the loaded model\n",
    "    y_pred = model.predict(combined_embeddings)\n",
    "    \n",
    "    # Get instrument names\n",
    "    sample_by_sample_predicted, aggregated_predicted_instruments = get_instruments(y_pred, classes, mapped_instruments)\n",
    "    \n",
    "    return sample_by_sample_predicted, aggregated_predicted_instruments\n",
    "# Example usage\n",
    "file_path = output_dir+'/converted_audio/test_data/1819_pcm.wav'\n",
    "predicted_instruments, aggregated_predicted_instruments = predict_instrument(file_path, clf, model, mapped_instruments)\n",
    "print(\"Predicted instruments:\", ', '.join(aggregated_predicted_instruments))\n",
    "# Print predicted value\n",
    "predicted_value = predicted_instruments[0][0] if predicted_instruments[0].size > 0 else \"No instrument\"\n",
    "print(f\"Predicted: {predicted_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('instrument_classifier_model.h5')\n",
    "\n",
    "# Save the scaler\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save the label encoder\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1167622,
     "sourceId": 1956211,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
